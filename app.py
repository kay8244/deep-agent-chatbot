"""
Deep Agent ë¦¬ì„œì¹˜ ì±—ë´‡ - Streamlit êµ¬í˜„

ì›¹ ê²€ìƒ‰, ìš”ì•½, ì„œë¸Œì—ì´ì „íŠ¸ ìœ„ì„ ê¸°ëŠ¥ì„ í¬í•¨í•œ ë¦¬ì„œì¹˜ ì—ì´ì „íŠ¸ì˜ ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤
ê¸°ì¡´ deep_agents_from_scratch íŒ¨í‚¤ì§€ì˜ ëª¨ë“ˆì„ ì¬ì‚¬ìš©í•˜ì—¬ ì½”ë“œ ì¤‘ë³µì„ ì œê±°í•©ë‹ˆë‹¤.

ê¸°ëŠ¥:
- ì¼ë°˜ ëŒ€í™”: LLM ì§ì ‘ ì‘ë‹µ (ë¹ ë¥¸ ë‹µë³€)
- ë”¥ ë¦¬ì„œì¹˜: ì›¹ ê²€ìƒ‰ + ì„œë¸Œì—ì´ì „íŠ¸ ìœ„ì„ (ì‹¬ì¸µ ì¡°ì‚¬, ì¶œì²˜ í¬í•¨)
"""

import json
import os
import re
from pathlib import Path
import streamlit as st
from datetime import datetime
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ: .env (ë¡œì»¬) â†’ Streamlit Cloud secrets (ë°°í¬) ìˆœì„œë¡œ ì‹œë„
load_dotenv(override=True)

# Streamlit Cloud secrets â†’ os.environ ìœ¼ë¡œ ê°•ì œ ì „ë‹¬ (override)
for key in ("ANTHROPIC_API_KEY", "TAVILY_API_KEY"):
    try:
        if key in st.secrets:
            os.environ[key] = st.secrets[key]
    except Exception:
        pass

from tavily import TavilyClient
from langchain.chat_models import init_chat_model
from langchain.agents import create_agent
from langchain_core.messages import HumanMessage, AIMessage

from deep_agents_from_scratch.state import DeepAgentState
from deep_agents_from_scratch.file_tools import ls, read_file, write_file
from deep_agents_from_scratch.todo_tools import write_todos, read_todos
from deep_agents_from_scratch.research_tools import tavily_search, think_tool
import deep_agents_from_scratch.research_tools as _research_tools
from deep_agents_from_scratch.task_tool import _create_task_tool

# íŒ¨í‚¤ì§€ ë‚´ë¶€ì˜ ëª¨ë“ˆ ë ˆë²¨ í´ë¼ì´ì–¸íŠ¸ë¥¼ ì˜¬ë°”ë¥¸ API í‚¤ë¡œ ì¬ì´ˆê¸°í™”
_research_tools.tavily_client = TavilyClient(
    api_key=os.environ.get("TAVILY_API_KEY", "")
)
_research_tools.summarization_model = init_chat_model(
    model="anthropic:claude-3-5-haiku-20241022",
    temperature=0.0,
    api_key=os.environ.get("ANTHROPIC_API_KEY"),
)
from deep_agents_from_scratch.prompts import (
    FILE_USAGE_INSTRUCTIONS,
    RESEARCHER_INSTRUCTIONS,
    SUBAGENT_USAGE_INSTRUCTIONS,
    TODO_USAGE_INSTRUCTIONS,
)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Deep Agent ë¦¬ì„œì¹˜ ì±—ë´‡",
    page_icon="ğŸ§ ",
    layout="wide",
)

# â”€â”€ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "messages" not in st.session_state:
    st.session_state.messages = []  # [{role, content, sources?, mode?}]
if "files" not in st.session_state:
    st.session_state.files = {}
if "research_stage" not in st.session_state:
    st.session_state.research_stage = "idle"  # "idle" | "plan_pending" | "follow_up"
if "pending_plan" not in st.session_state:
    st.session_state.pending_plan = ""
if "pending_query" not in st.session_state:
    st.session_state.pending_query = ""


# â”€â”€ ìºì‹œëœ ë¦¬ì†ŒìŠ¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource
def _init_model():
    """ë©”ì¸ LLMì„ í•œ ë²ˆë§Œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    return init_chat_model(
        model="anthropic:claude-haiku-4-5-20251001",
        temperature=0.0,
        api_key=os.environ.get("ANTHROPIC_API_KEY"),
    )


@st.cache_resource
def _create_agent():
    """ë¦¬ì„œì¹˜ ì—ì´ì „íŠ¸ë¥¼ í•œ ë²ˆë§Œ ìƒì„±í•©ë‹ˆë‹¤."""
    model = _init_model()
    now = datetime.now()

    sub_agent_tools = [tavily_search, think_tool]
    built_in_tools = [ls, read_file, write_file, write_todos, read_todos, think_tool]

    research_sub_agent = {
        "name": "research-agent",
        "description": (
            "Delegate research to the sub-agent researcher. "
            "Only give this researcher one topic at a time."
        ),
        "prompt": RESEARCHER_INSTRUCTIONS.format(
            date=now.strftime("%b %-d, %Y %H:%M:%S (%A)")
        ),
        "tools": ["tavily_search", "think_tool"],
    }

    task_tool = _create_task_tool(
        sub_agent_tools, [research_sub_agent], model, DeepAgentState
    )

    all_tools = sub_agent_tools + built_in_tools + [task_tool]

    subagent_instructions = SUBAGENT_USAGE_INSTRUCTIONS.format(
        max_concurrent_research_units=1,
        max_researcher_iterations=1,
        date=now.strftime("%a %b %-d, %Y"),
    )

    citation_instructions = (
        "# CITATION RULES\n"
        "When writing the final report, you MUST follow these citation rules:\n"
        "- After each factual claim, add an inline citation linking to the source URL.\n"
        "- Use markdown link format: `ë¬¸ì¥ ë‚´ìš© ([ì¶œì²˜ì œëª©](URL))`\n"
        "- Example: 2024ë…„ AI ì‹œì¥ ê·œëª¨ëŠ” 1ì¡° ë‹¬ëŸ¬ì— ë‹¬í–ˆë‹¤ ([Forbes](https://forbes.com/...)).\n"
        "- Every fact must have at least one citation. Do not omit citations.\n"
        "- At the end of the report, include a numbered '## ì°¸ê³  ë¬¸í—Œ' section listing all sources.\n"
    )

    system_prompt = "\n\n".join(
        [
            "# TODO MANAGEMENT",
            TODO_USAGE_INSTRUCTIONS,
            "=" * 80,
            "# FILE SYSTEM USAGE",
            FILE_USAGE_INSTRUCTIONS,
            "=" * 80,
            "# SUB-AGENT DELEGATION",
            subagent_instructions,
            "=" * 80,
            citation_instructions,
        ]
    )

    return create_agent(
        model, all_tools, system_prompt=system_prompt, state_schema=DeepAgentState
    )


# â”€â”€ ë¦¬ì„œì¹˜ ê³„íš ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _generate_plan(query: str) -> str:
    """ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°›ì•„ ë¦¬ì„œì¹˜ ê³„íšë§Œ ìƒì„±í•©ë‹ˆë‹¤ (ì‹¤ì œ ë¦¬ì„œì¹˜ëŠ” ìˆ˜í–‰í•˜ì§€ ì•ŠìŒ)."""
    model = _init_model()
    today = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
    plan_prompt = (
        f"ì˜¤ëŠ˜ ë‚ ì§œ: {today}\n\n"
        "ë‹¹ì‹ ì€ ë¦¬ì„œì¹˜ í”Œë˜ë„ˆì…ë‹ˆë‹¤. ì•„ë˜ ì§ˆë¬¸ì— ëŒ€í•´ ë¦¬ì„œì¹˜ ê³„íšë§Œ ì‘ì„±í•˜ì„¸ìš”.\n"
        "ì‹¤ì œ ë¦¬ì„œì¹˜ëŠ” ìˆ˜í–‰í•˜ì§€ ë§ˆì„¸ìš”.\n\n"
        "ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë²ˆí˜¸ ë§¤ê¸´ ë‹¨ê³„ë³„ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”:\n"
        "1. [ë‹¨ê³„ ì„¤ëª…]\n"
        "2. [ë‹¨ê³„ ì„¤ëª…]\n"
        "...\n\n"
        f"ì§ˆë¬¸: {query}"
    )
    with st.spinner("ğŸ“‹ ë¦¬ì„œì¹˜ ê³„íš ìƒì„± ì¤‘..."):
        response = model.invoke([HumanMessage(content=plan_prompt)])
    if isinstance(response.content, str):
        return response.content
    parts = [
        item["text"]
        for item in response.content
        if isinstance(item, dict) and item.get("type") == "text"
    ]
    return "\n".join(parts) if parts else str(response.content)


# â”€â”€ ìœ í‹¸ë¦¬í‹° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _extract_ai_response(messages: list) -> str:
    """ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì—ì„œ ë§ˆì§€ë§‰ AI ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    for msg in reversed(messages):
        if not isinstance(msg, AIMessage) or not msg.content:
            continue
        if isinstance(msg.content, str):
            return msg.content
        parts = [
            item["text"]
            for item in msg.content
            if isinstance(item, dict) and item.get("type") == "text"
        ]
        if parts:
            return "\n".join(parts)
    return "ë¦¬ì„œì¹˜ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ì €ì¥ëœ íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”."


def _to_langchain_messages(history: list[dict]) -> list:
    """Streamlit ì±„íŒ… íˆìŠ¤í† ë¦¬ë¥¼ LangChain ë©”ì‹œì§€ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    return [
        HumanMessage(content=m["content"])
        if m["role"] == "user"
        else AIMessage(content=m["content"])
        for m in history
    ]


def _extract_sources(files: dict) -> list[dict]:
    """íŒŒì¼ë“¤ì—ì„œ ì¶œì²˜(URL, ì œëª©) ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    sources = []
    seen_urls = set()
    for content in files.values():
        url_match = re.search(r"\*\*URL:\*\*\s*(https?://\S+)", content)
        title_match = re.search(r"# Search Result:\s*(.+)", content)
        if url_match:
            url = url_match.group(1)
            if url not in seen_urls:
                seen_urls.add(url)
                title = title_match.group(1).strip() if title_match else url
                sources.append({"title": title, "url": url})
    return sources


LOCAL_SAVE_DIR = Path("research_outputs")
TEST_CACHE_FILE = LOCAL_SAVE_DIR / "_last_research_cache.json"


def _save_research_cache(response: str, files: dict, sources: list[dict]):
    """ë§ˆì§€ë§‰ ë¦¬ì„œì¹˜ ê²°ê³¼ë¥¼ JSON ìºì‹œ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
    LOCAL_SAVE_DIR.mkdir(exist_ok=True)
    cache = {"response": response, "files": files, "sources": sources}
    TEST_CACHE_FILE.write_text(json.dumps(cache, ensure_ascii=False, indent=2), encoding="utf-8")


def _load_research_cache() -> tuple[str, dict, list[dict]] | None:
    """ìºì‹œëœ ë¦¬ì„œì¹˜ ê²°ê³¼ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. ì—†ìœ¼ë©´ None ë°˜í™˜."""
    if not TEST_CACHE_FILE.exists():
        return None
    cache = json.loads(TEST_CACHE_FILE.read_text(encoding="utf-8"))
    return cache["response"], cache["files"], cache["sources"]


def _sanitize_folder_name(query: str) -> str:
    """ì§ˆë¬¸ í…ìŠ¤íŠ¸ë¥¼ í´ë”ëª…ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    # íŒŒì¼ì‹œìŠ¤í…œì— ì•ˆì „í•˜ì§€ ì•Šì€ ë¬¸ì ì œê±°
    safe = re.sub(r'[\\/:*?"<>|]', "", query)
    # ê³µë°± ì •ë¦¬ ë° ê¸¸ì´ ì œí•œ
    safe = safe.strip()[:50].strip()
    return safe or "research"


def _save_files_to_disk(files: dict, query: str = ""):
    """ê°€ìƒ íŒŒì¼ì‹œìŠ¤í…œì˜ íŒŒì¼ë“¤ì„ ë¡œì»¬ ë””ìŠ¤í¬ì— ìë™ ì €ì¥í•©ë‹ˆë‹¤.

    research_outputs/<ì§ˆë¬¸ìš”ì•½>/ í•˜ìœ„ì— ë²ˆí˜¸ ë§¤ê¸´ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    if not files:
        return
    folder_name = _sanitize_folder_name(query) if query else "research"
    save_dir = LOCAL_SAVE_DIR / folder_name
    save_dir.mkdir(parents=True, exist_ok=True)
    for idx, (fname, content) in enumerate(files.items(), 1):
        safe_name = Path(fname).name
        numbered_name = f"{idx:02d}_{safe_name}"
        filepath = save_dir / numbered_name
        filepath.write_text(content, encoding="utf-8")


def _render_sources(sources: list[dict]):
    """ì¶œì²˜ ëª©ë¡ì„ ë Œë”ë§í•©ë‹ˆë‹¤."""
    if not sources:
        return
    with st.expander(f"ğŸ“š ì¶œì²˜ ({len(sources)}ê±´)", expanded=False):
        for i, src in enumerate(sources, 1):
            st.markdown(f"{i}. [{src['title']}]({src['url']})")


# â”€â”€ ì‚¬ì´ë“œë°” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _render_sidebar() -> tuple[str, bool]:
    """ì‚¬ì´ë“œë°”ë¥¼ ë Œë”ë§í•˜ê³  (ëª¨ë“œ, í…ŒìŠ¤íŠ¸ëª¨ë“œ ì—¬ë¶€)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")

        # ëª¨ë“œ ì„ íƒ
        mode = st.radio(
            "ëŒ€í™” ëª¨ë“œ",
            options=["ì¼ë°˜ ëŒ€í™”", "ë”¥ ë¦¬ì„œì¹˜"],
            index=0,
            help="ì¼ë°˜ ëŒ€í™”: ë¹ ë¥¸ LLM ì§ì ‘ ì‘ë‹µ\në”¥ ë¦¬ì„œì¹˜: ì›¹ ê²€ìƒ‰ + ì„œë¸Œì—ì´ì „íŠ¸ ì‹¬ì¸µ ì¡°ì‚¬",
        )

        test_mode = st.toggle(
            "ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ",
            value=False,
            help="ì¼œë©´ API í˜¸ì¶œ ì—†ì´ ë§ˆì§€ë§‰ ìºì‹œëœ ë¦¬ì„œì¹˜ ê²°ê³¼ë¥¼ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.",
        )
        if test_mode:
            has_cache = TEST_CACHE_FILE.exists()
            if has_cache:
                st.caption("âœ… ìºì‹œ íŒŒì¼ ìˆìŒ â€” API í˜¸ì¶œ ì—†ì´ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥")
            else:
                st.caption("âš ï¸ ìºì‹œ ì—†ìŒ â€” ë¨¼ì € ë”¥ ë¦¬ì„œì¹˜ë¥¼ 1íšŒ ì‹¤í–‰í•˜ì„¸ìš”")

        st.divider()

        if st.button("ğŸ—‘ï¸ ì±„íŒ… ê¸°ë¡ ì‚­ì œ", use_container_width=True):
            st.session_state.messages = []
            st.session_state.files = {}
            st.session_state.research_stage = "idle"
            st.session_state.pending_plan = ""
            st.session_state.pending_query = ""
            st.rerun()

        st.divider()
        st.header("ğŸ“ ì €ì¥ëœ íŒŒì¼")

        if st.session_state.files:
            for fname, content in st.session_state.files.items():
                with st.expander(fname):
                    st.code(content, language="markdown")
                    st.download_button(
                        label=f"â¬‡ï¸ {fname} ë‹¤ìš´ë¡œë“œ",
                        data=content,
                        file_name=Path(fname).name,
                        mime="text/markdown",
                        key=f"dl_{fname}",
                    )
            st.caption(f"ğŸ“‚ ìë™ ì €ì¥ ê²½ë¡œ: `{LOCAL_SAVE_DIR.resolve()}`")
        else:
            st.info("ì•„ì§ ì €ì¥ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    return mode, test_mode


# â”€â”€ ì¼ë°˜ ëŒ€í™” ì‹¤í–‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _run_normal_chat(history: list[dict]) -> str:
    """LLMì— ì§ì ‘ ì§ˆë¬¸í•˜ì—¬ ë¹ ë¥¸ ì‘ë‹µì„ ë°›ìŠµë‹ˆë‹¤."""
    model = _init_model()
    lc_messages = _to_langchain_messages(history)
    with st.spinner("ğŸ’¬ ë‹µë³€ ìƒì„± ì¤‘..."):
        response = model.invoke(lc_messages)
    if isinstance(response.content, str):
        return response.content
    parts = [
        item["text"]
        for item in response.content
        if isinstance(item, dict) and item.get("type") == "text"
    ]
    return "\n".join(parts) if parts else str(response.content)


# â”€â”€ í›„ì† ëŒ€í™” (ë¦¬ì„œì¹˜ ê²°ê³¼ ê¸°ë°˜) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _extract_all_urls(content: str) -> list[tuple[str, str]]:
    """íŒŒì¼ ë‚´ìš©ì—ì„œ ëª¨ë“  (ì œëª©, URL) ìŒì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    urls = []
    seen = set()
    # **URL:** íŒ¨í„´
    for m in re.finditer(r"\*\*URL:\*\*\s*(https?://\S+)", content):
        url = m.group(1)
        if url not in seen:
            seen.add(url)
            urls.append(url)
    # markdown ë§í¬ íŒ¨í„´ [title](url)
    for m in re.finditer(r"\[([^\]]+)\]\((https?://[^\)]+)\)", content):
        url = m.group(2)
        if url not in seen:
            seen.add(url)
            urls.append(url)
    return urls


def _build_source_map(files: dict) -> str:
    """ëª¨ë“  íŒŒì¼ì—ì„œ URLì„ ì¶”ì¶œí•˜ì—¬ ì¶œì²˜ ë§¤í•‘ í…Œì´ë¸”ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    all_urls = {}  # url -> set of file names
    for fname, content in files.items():
        for url in _extract_all_urls(content):
            all_urls.setdefault(url, set()).add(fname)

    if not all_urls:
        return ""

    lines = ["## ì¶œì²˜ URL ëª©ë¡ (ì¸ë¼ì¸ ì¶œì²˜ì— ë°˜ë“œì‹œ ì´ URLì„ ì‚¬ìš©í•˜ì„¸ìš”)"]
    for i, (url, fnames) in enumerate(all_urls.items(), 1):
        lines.append(f"{i}. {url} (ê´€ë ¨ íŒŒì¼: {', '.join(fnames)})")
    return "\n".join(lines)


def _build_file_context(files: dict, max_chars: int = 50000) -> str:
    """ë¦¬ì„œì¹˜ íŒŒì¼ ë‚´ìš©ì„ LLM ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    final/report/findings íŒŒì¼ì„ ìš°ì„  í¬í•¨í•˜ê³ , ë‚˜ë¨¸ì§€ëŠ” ê³µê°„ì´ ë‚¨ìœ¼ë©´ ì¶”ê°€í•©ë‹ˆë‹¤.
    """
    if not files:
        return ""

    # ì¶œì²˜ ë§¤í•‘ í…Œì´ë¸”ì„ ë¨¼ì € í¬í•¨
    source_map = _build_source_map(files)

    # ìš°ì„ ìˆœìœ„ íŒŒì¼ ë¶„ë¥˜
    priority_keywords = ("final", "report", "findings", "comprehensive")
    priority_files = {}
    other_files = {}
    for fname, content in files.items():
        fname_lower = fname.lower()
        if any(kw in fname_lower for kw in priority_keywords):
            priority_files[fname] = content
        else:
            other_files[fname] = content

    context_parts = [source_map] if source_map else []
    total_chars = len(source_map)

    for group in [priority_files, other_files]:
        for fname, content in group.items():
            urls = _extract_all_urls(content)
            url_line = "ì¶œì²˜ URLs: " + ", ".join(urls) if urls else "ì¶œì²˜ URL: ì—†ìŒ (ì—ì´ì „íŠ¸ ìƒì„± ìš”ì•½)"
            entry = f"### íŒŒì¼: {fname}\n{url_line}\n{content}\n"
            if total_chars + len(entry) > max_chars:
                break
            context_parts.append(entry)
            total_chars += len(entry)

    return "\n".join(context_parts)


def _run_follow_up_chat(history: list[dict], files: dict) -> str:
    """ë¦¬ì„œì¹˜ ê²°ê³¼ íŒŒì¼ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ í¬í•¨í•˜ì—¬ í›„ì† ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤."""
    model = _init_model()
    file_context = _build_file_context(files)

    today = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
    system_msg = (
        f"ì˜¤ëŠ˜ ë‚ ì§œ: {today}\n\n"
        "ë‹¹ì‹ ì€ ë¦¬ì„œì¹˜ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í›„ì† ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
        "ì•„ë˜ì— ë¦¬ì„œì¹˜ì—ì„œ ìˆ˜ì§‘ëœ íŒŒì¼ ë‚´ìš©ì´ ì œê³µë©ë‹ˆë‹¤. "
        "ì´ ìë£Œë¥¼ ê·¼ê±°ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n\n"
        "## ì¶œì²˜ í‘œê¸° ê·œì¹™ (í•„ìˆ˜)\n"
        "- ëª¨ë“  ì‚¬ì‹¤, ìˆ˜ì¹˜, í†µê³„ì—ëŠ” ë°˜ë“œì‹œ ì¸ë¼ì¸ ì¶œì²˜ë¥¼ ë‹¬ì•„ì•¼ í•©ë‹ˆë‹¤.\n"
        "- í˜•ì‹: ë¬¸ì¥ ë‚´ìš© ([ì¶œì²˜ì œëª©](URL))\n"
        "- ì˜ˆì‹œ: DRAM ê°€ê²©ì´ 15% ìƒìŠ¹í–ˆë‹¤ ([TrendForce](https://trendforce.com/...)).\n"
        "- ë°˜ë“œì‹œ 'ì¶œì²˜ URL ëª©ë¡'ì— ìˆëŠ” ì‹¤ì œ URLì„ ì‚¬ìš©í•˜ì„¸ìš”. íŒŒì¼ëª…ì„ ì¶œì²˜ë¡œ ì“°ì§€ ë§ˆì„¸ìš”.\n"
        "- ì„œë¡œ ë‹¤ë¥¸ ì‚¬ì‹¤ì—ëŠ” í•´ë‹¹ ë‚´ìš©ì´ í¬í•¨ëœ ì„œë¡œ ë‹¤ë¥¸ ì¶œì²˜ URLì„ ë§¤ì¹­í•˜ì„¸ìš”.\n"
        "- ë‹µë³€ ë§ˆì§€ë§‰ì— '## ì°¸ê³  ë¬¸í—Œ' ì„¹ì…˜ì„ ì¶”ê°€í•˜ì—¬ ì‚¬ìš©í•œ ì¶œì²˜ë¥¼ ë²ˆí˜¸ ë§¤ê²¨ ë‚˜ì—´í•˜ì„¸ìš”.\n\n"
        f"## ë¦¬ì„œì¹˜ ìë£Œ\n\n{file_context}"
    )

    lc_messages = [HumanMessage(content=system_msg)] + _to_langchain_messages(history)
    with st.spinner("ğŸ’¬ ë‹µë³€ ìƒì„± ì¤‘..."):
        response = model.invoke(lc_messages)
    if isinstance(response.content, str):
        return response.content
    parts = [
        item["text"]
        for item in response.content
        if isinstance(item, dict) and item.get("type") == "text"
    ]
    return "\n".join(parts) if parts else str(response.content)


# â”€â”€ ë”¥ ë¦¬ì„œì¹˜ ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¬ë°) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _run_deep_research(agent, state: dict) -> tuple[str, dict, list[dict]]:
    """ì—ì´ì „íŠ¸ë¥¼ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì‹¤í–‰í•˜ê³  ì§„í–‰ ìƒí™©ì„ í‘œì‹œí•©ë‹ˆë‹¤.

    Returns:
        (ì‘ë‹µ í…ìŠ¤íŠ¸, ìµœì¢… íŒŒì¼ dict, ì¶œì²˜ ë¦¬ìŠ¤íŠ¸)
    """
    final_state = None
    tool_calls_shown = set()
    files_before = set(state.get("files", {}).keys())

    with st.status("ğŸ” ë”¥ ë¦¬ì„œì¹˜ ì§„í–‰ ì¤‘...", expanded=True) as status:
        for event in agent.stream(state, stream_mode="values"):
            final_state = event

            for msg in event.get("messages", []):
                if not isinstance(msg, AIMessage):
                    continue
                for tc in getattr(msg, "tool_calls", []) or []:
                    tc_id = tc.get("id", "")
                    if tc_id not in tool_calls_shown:
                        tool_calls_shown.add(tc_id)
                        name = tc.get("name", "unknown")
                        args = tc.get("args", {})
                        detail = ""
                        if "query" in args:
                            detail = f' â†’ "{args["query"]}"'
                        elif "description" in args:
                            desc = args["description"]
                            detail = (
                                f" â†’ {desc[:60]}..."
                                if len(desc) > 60
                                else f" â†’ {desc}"
                            )
                        st.write(f"ğŸ”§ `{name}`{detail}")

        status.update(label="âœ… ë¦¬ì„œì¹˜ ì™„ë£Œ", state="complete")

    if final_state is None:
        return "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", state.get("files", {}), []

    response = _extract_ai_response(final_state.get("messages", []))
    files = final_state.get("files", state.get("files", {}))

    # ì´ë²ˆ ë¦¬ì„œì¹˜ì—ì„œ ìƒˆë¡œ ìƒì„±ëœ íŒŒì¼ì—ì„œë§Œ ì¶œì²˜ ì¶”ì¶œ
    new_files = {k: v for k, v in files.items() if k not in files_before}
    sources = _extract_sources(new_files) if new_files else _extract_sources(files)

    return response, files, sources


# â”€â”€ ë©”ì‹œì§€ ë Œë”ë§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _render_message(msg: dict):
    """ë©”ì‹œì§€ í•˜ë‚˜ë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤ (ì¶œì²˜ í¬í•¨)."""
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])
        if msg.get("sources"):
            _render_sources(msg["sources"])
        if msg.get("mode") == "ë”¥ ë¦¬ì„œì¹˜":
            st.caption("ğŸ”¬ ë”¥ ë¦¬ì„œì¹˜")


# â”€â”€ ë©”ì¸ ì•± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    st.title("ğŸ§  Deep Agent ë¦¬ì„œì¹˜ ì±—ë´‡")
    st.caption("ì›¹ ê²€ìƒ‰ Â· ìš”ì•½ Â· ì„œë¸Œì—ì´ì „íŠ¸ ìœ„ì„ ê¸°ëŠ¥ì„ ê°–ì¶˜ ë¦¬ì„œì¹˜ ì—ì´ì „íŠ¸")

    mode, test_mode = _render_sidebar()

    # ì±„íŒ… íˆìŠ¤í† ë¦¬ í‘œì‹œ
    for msg in st.session_state.messages:
        _render_message(msg)

    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if mode == "ë”¥ ë¦¬ì„œì¹˜" and st.session_state.research_stage == "plan_pending":
        placeholder = "ìŠ¹ì¸(ì§„í–‰/ë„¤/ok) ë˜ëŠ” ìˆ˜ì • ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”..."
    elif mode == "ë”¥ ë¦¬ì„œì¹˜" and st.session_state.research_stage == "follow_up":
        placeholder = "í›„ì† ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”... (ìƒˆ ì£¼ì œëŠ” 'ìƒˆ ë¦¬ì„œì¹˜'ë¥¼ ì…ë ¥)"
    elif mode == "ë”¥ ë¦¬ì„œì¹˜":
        placeholder = "ë¦¬ì„œì¹˜í•  ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”..."
    else:
        placeholder = "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."

    _needs_rerun = False

    if prompt := st.chat_input(placeholder):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            try:
                if mode == "ì¼ë°˜ ëŒ€í™”":
                    response = _run_normal_chat(st.session_state.messages)
                    st.markdown(response)
                    st.session_state.messages.append(
                        {"role": "assistant", "content": response}
                    )

                elif st.session_state.research_stage == "follow_up":
                    # ë”¥ ë¦¬ì„œì¹˜: í›„ì† ëŒ€í™” ë‹¨ê³„
                    new_research_keywords = {"ìƒˆ ë¦¬ì„œì¹˜", "ìƒˆë¦¬ì„œì¹˜", "new research", "ìƒˆë¡œìš´ ë¦¬ì„œì¹˜"}
                    if prompt.strip().lower() in new_research_keywords:
                        st.session_state.research_stage = "idle"
                        st.session_state.files = {}
                        msg = "ìƒˆë¡œìš´ ë¦¬ì„œì¹˜ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. ë¦¬ì„œì¹˜í•  ì£¼ì œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."
                        st.markdown(msg)
                        st.session_state.messages.append(
                            {"role": "assistant", "content": msg}
                        )
                    else:
                        response = _run_follow_up_chat(
                            st.session_state.messages, st.session_state.files
                        )
                        st.markdown(response)
                        st.session_state.messages.append(
                            {"role": "assistant", "content": response}
                        )

                elif st.session_state.research_stage == "idle" and test_mode:
                    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ìºì‹œì—ì„œ ë°”ë¡œ ê²°ê³¼ ë¡œë“œ (API í˜¸ì¶œ ì—†ìŒ)
                    cached = _load_research_cache()
                    if cached:
                        response, files, sources = cached
                        st.session_state.files = files
                        st.info("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ìºì‹œëœ ë¦¬ì„œì¹˜ ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.")
                        st.markdown(response)
                        if sources:
                            _render_sources(sources)
                        st.caption("ğŸ”¬ ë”¥ ë¦¬ì„œì¹˜")
                        st.session_state.messages.append(
                            {
                                "role": "assistant",
                                "content": response,
                                "sources": sources,
                                "mode": "ë”¥ ë¦¬ì„œì¹˜",
                            }
                        )
                        st.session_state.research_stage = "follow_up"
                        _needs_rerun = True
                    else:
                        msg = "âš ï¸ ìºì‹œëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ëª¨ë“œë¥¼ ë„ê³  ë”¥ ë¦¬ì„œì¹˜ë¥¼ 1íšŒ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
                        st.warning(msg)
                        st.session_state.messages.append(
                            {"role": "assistant", "content": msg}
                        )

                elif st.session_state.research_stage == "idle":
                    # ë”¥ ë¦¬ì„œì¹˜: ê³„íš ìƒì„± ë‹¨ê³„
                    plan = _generate_plan(prompt)
                    plan_message = (
                        f"**ğŸ“‹ ë¦¬ì„œì¹˜ ê³„íš**\n\n{plan}\n\n---\n"
                        "ì´ ê³„íšëŒ€ë¡œ ì§„í–‰í• ê¹Œìš”? "
                        "ìŠ¹ì¸í•˜ë ¤ë©´ **ì§„í–‰/ë„¤/ok** ë“±ì„ ì…ë ¥í•˜ê³ , "
                        "ìˆ˜ì •ì´ í•„ìš”í•˜ë©´ ìˆ˜ì • ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
                    )
                    st.markdown(plan_message)

                    st.session_state.messages.append(
                        {"role": "assistant", "content": plan_message}
                    )
                    st.session_state.research_stage = "plan_pending"
                    st.session_state.pending_plan = plan
                    st.session_state.pending_query = prompt

                else:
                    # ë”¥ ë¦¬ì„œì¹˜: ìŠ¹ì¸/ìˆ˜ì • ì²˜ë¦¬ ë‹¨ê³„
                    approval_keywords = {
                        "ì§„í–‰", "ë„¤", "ì¢‹ì•„", "ã…‡ã…‡", "ok", "yes",
                        "ì‘", "ì¢‹ì•„ìš”", "í™•ì¸", "ã…‡", "ê³ ", "ì‹œì‘",
                    }
                    user_input = prompt.strip().lower()

                    if user_input in approval_keywords:
                        plan = st.session_state.pending_plan
                    else:
                        plan = prompt  # ìˆ˜ì • ë‚´ìš©ì„ ìƒˆ ê³„íšìœ¼ë¡œ ì‚¬ìš©

                    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ + ìºì‹œ ìˆìŒ: ìºì‹œëœ ê²°ê³¼ ì‚¬ìš©
                    cached = _load_research_cache() if test_mode else None
                    if cached:
                        response, files, sources = cached
                        st.info("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ìºì‹œëœ ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.")
                    else:
                        # ì‹¤ì œ ë¦¬ì„œì¹˜ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ ëª¨ë“œì—¬ë„ ìºì‹œ ì—†ìœ¼ë©´ fallback)
                        if test_mode:
                            st.warning("ğŸ§ª ìºì‹œ ì—†ìŒ â€” ì‹¤ì œ ë¦¬ì„œì¹˜ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.")
                        research_prompt = (
                            f"ì‚¬ìš©ì ì§ˆë¬¸: {st.session_state.pending_query}\n\n"
                            f"ë¦¬ì„œì¹˜ ê³„íš:\n{plan}\n\n"
                            "ìœ„ ê³„íšì— ë”°ë¼ ë¦¬ì„œì¹˜ë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”."
                        )

                        agent = _create_agent()
                        agent_state = {
                            "messages": [HumanMessage(content=research_prompt)],
                            "files": st.session_state.files,
                        }

                        response, files, sources = _run_deep_research(
                            agent, agent_state
                        )
                        _save_files_to_disk(files, st.session_state.pending_query)
                        _save_research_cache(response, files, sources)

                    st.session_state.files = files

                    st.markdown(response)
                    if sources:
                        _render_sources(sources)
                    st.caption("ğŸ”¬ ë”¥ ë¦¬ì„œì¹˜")

                    st.session_state.messages.append(
                        {
                            "role": "assistant",
                            "content": response,
                            "sources": sources,
                            "mode": "ë”¥ ë¦¬ì„œì¹˜",
                        }
                    )

                    # ë¦¬ì„œì¹˜ ì™„ë£Œ â†’ í›„ì† ëŒ€í™” ëª¨ë“œë¡œ ì „í™˜
                    st.session_state.research_stage = "follow_up"
                    st.session_state.pending_plan = ""
                    st.session_state.pending_query = ""
                    _needs_rerun = True

            except Exception as e:
                error_msg = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                st.error(error_msg)
                st.exception(e)
                st.session_state.messages.append(
                    {"role": "assistant", "content": error_msg}
                )

    # try/except ë°–ì—ì„œ reruní•˜ì—¬ RerunExceptionì´ ì¡íˆì§€ ì•Šë„ë¡ í•¨
    if _needs_rerun:
        st.rerun()


if __name__ == "__main__":
    main()
