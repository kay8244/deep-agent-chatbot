"""
Deep Agent ë¦¬ì„œì¹˜ ì±—ë´‡ - Streamlit êµ¬í˜„

ì›¹ ê²€ìƒ‰, ìš”ì•½, ì„œë¸Œì—ì´ì „íŠ¸ ìœ„ì„ ê¸°ëŠ¥ì„ í¬í•¨í•œ ë¦¬ì„œì¹˜ ì—ì´ì „íŠ¸ì˜ ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤
ê¸°ì¡´ deep_agents_from_scratch íŒ¨í‚¤ì§€ì˜ ëª¨ë“ˆì„ ì¬ì‚¬ìš©í•˜ì—¬ ì½”ë“œ ì¤‘ë³µì„ ì œê±°í•©ë‹ˆë‹¤.

ê¸°ëŠ¥:
- ì¼ë°˜ ëŒ€í™”: LLM ì§ì ‘ ì‘ë‹µ (ë¹ ë¥¸ ë‹µë³€)
- ë”¥ ë¦¬ì„œì¹˜: ì›¹ ê²€ìƒ‰ + ì„œë¸Œì—ì´ì „íŠ¸ ìœ„ì„ (ì‹¬ì¸µ ì¡°ì‚¬, ì¶œì²˜ í¬í•¨)
"""

import os
import re
import streamlit as st
from datetime import datetime
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ: .env (ë¡œì»¬) â†’ Streamlit Cloud secrets (ë°°í¬) ìˆœì„œë¡œ ì‹œë„
load_dotenv(override=True)

# Streamlit Cloud secrets â†’ os.environ ìœ¼ë¡œ ê°•ì œ ì „ë‹¬ (override)
for key in ("ANTHROPIC_API_KEY", "TAVILY_API_KEY"):
    try:
        if key in st.secrets:
            os.environ[key] = st.secrets[key]
    except Exception:
        pass

# API í‚¤ ë¡œë“œ í™•ì¸ (ì‚¬ì´ë“œë°”ì— ìƒíƒœ í‘œì‹œìš©)
_api_key_status = {
    "ANTHROPIC_API_KEY": bool(os.environ.get("ANTHROPIC_API_KEY")),
    "TAVILY_API_KEY": bool(os.environ.get("TAVILY_API_KEY")),
}

from langchain.chat_models import init_chat_model
from langchain.agents import create_agent
from langchain_core.messages import HumanMessage, AIMessage

from deep_agents_from_scratch.state import DeepAgentState
from deep_agents_from_scratch.file_tools import ls, read_file, write_file
from deep_agents_from_scratch.todo_tools import write_todos, read_todos
from deep_agents_from_scratch.research_tools import tavily_search, think_tool
from deep_agents_from_scratch.task_tool import _create_task_tool
from deep_agents_from_scratch.prompts import (
    FILE_USAGE_INSTRUCTIONS,
    RESEARCHER_INSTRUCTIONS,
    SUBAGENT_USAGE_INSTRUCTIONS,
    TODO_USAGE_INSTRUCTIONS,
)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Deep Agent ë¦¬ì„œì¹˜ ì±—ë´‡",
    page_icon="ğŸ§ ",
    layout="wide",
)

# â”€â”€ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "messages" not in st.session_state:
    st.session_state.messages = []  # [{role, content, sources?, mode?}]
if "files" not in st.session_state:
    st.session_state.files = {}


# â”€â”€ ìºì‹œëœ ë¦¬ì†ŒìŠ¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource
def _init_model():
    """ë©”ì¸ LLMì„ í•œ ë²ˆë§Œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    return init_chat_model(
        model="anthropic:claude-sonnet-4-5",
        temperature=0.0,
        api_key=os.environ.get("ANTHROPIC_API_KEY"),
    )


@st.cache_resource
def _create_agent():
    """ë¦¬ì„œì¹˜ ì—ì´ì „íŠ¸ë¥¼ í•œ ë²ˆë§Œ ìƒì„±í•©ë‹ˆë‹¤."""
    model = _init_model()
    now = datetime.now()

    sub_agent_tools = [tavily_search, think_tool]
    built_in_tools = [ls, read_file, write_file, write_todos, read_todos, think_tool]

    research_sub_agent = {
        "name": "research-agent",
        "description": (
            "Delegate research to the sub-agent researcher. "
            "Only give this researcher one topic at a time."
        ),
        "prompt": RESEARCHER_INSTRUCTIONS.format(
            date=now.strftime("%b %-d, %Y %H:%M:%S (%A)")
        ),
        "tools": ["tavily_search", "think_tool"],
    }

    task_tool = _create_task_tool(
        sub_agent_tools, [research_sub_agent], model, DeepAgentState
    )

    all_tools = sub_agent_tools + built_in_tools + [task_tool]

    subagent_instructions = SUBAGENT_USAGE_INSTRUCTIONS.format(
        max_concurrent_research_units=3,
        max_researcher_iterations=3,
        date=now.strftime("%a %b %-d, %Y"),
    )

    system_prompt = "\n\n".join(
        [
            "# TODO MANAGEMENT",
            TODO_USAGE_INSTRUCTIONS,
            "=" * 80,
            "# FILE SYSTEM USAGE",
            FILE_USAGE_INSTRUCTIONS,
            "=" * 80,
            "# SUB-AGENT DELEGATION",
            subagent_instructions,
        ]
    )

    return create_agent(
        model, all_tools, system_prompt=system_prompt, state_schema=DeepAgentState
    )


# â”€â”€ ìœ í‹¸ë¦¬í‹° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _extract_ai_response(messages: list) -> str:
    """ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì—ì„œ ë§ˆì§€ë§‰ AI ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    for msg in reversed(messages):
        if not isinstance(msg, AIMessage) or not msg.content:
            continue
        if isinstance(msg.content, str):
            return msg.content
        parts = [
            item["text"]
            for item in msg.content
            if isinstance(item, dict) and item.get("type") == "text"
        ]
        if parts:
            return "\n".join(parts)
    return "ë¦¬ì„œì¹˜ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ì €ì¥ëœ íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”."


def _to_langchain_messages(history: list[dict]) -> list:
    """Streamlit ì±„íŒ… íˆìŠ¤í† ë¦¬ë¥¼ LangChain ë©”ì‹œì§€ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    return [
        HumanMessage(content=m["content"])
        if m["role"] == "user"
        else AIMessage(content=m["content"])
        for m in history
    ]


def _extract_sources(files: dict) -> list[dict]:
    """íŒŒì¼ë“¤ì—ì„œ ì¶œì²˜(URL, ì œëª©) ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    sources = []
    seen_urls = set()
    for content in files.values():
        url_match = re.search(r"\*\*URL:\*\*\s*(https?://\S+)", content)
        title_match = re.search(r"# Search Result:\s*(.+)", content)
        if url_match:
            url = url_match.group(1)
            if url not in seen_urls:
                seen_urls.add(url)
                title = title_match.group(1).strip() if title_match else url
                sources.append({"title": title, "url": url})
    return sources


def _render_sources(sources: list[dict]):
    """ì¶œì²˜ ëª©ë¡ì„ ë Œë”ë§í•©ë‹ˆë‹¤."""
    if not sources:
        return
    with st.expander(f"ğŸ“š ì¶œì²˜ ({len(sources)}ê±´)", expanded=False):
        for i, src in enumerate(sources, 1):
            st.markdown(f"{i}. [{src['title']}]({src['url']})")


# â”€â”€ ì‚¬ì´ë“œë°” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _render_sidebar() -> str:
    """ì‚¬ì´ë“œë°”ë¥¼ ë Œë”ë§í•˜ê³  ì„ íƒëœ ëª¨ë“œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")

        mode = st.radio(
            "ëŒ€í™” ëª¨ë“œ",
            options=["ì¼ë°˜ ëŒ€í™”", "ë”¥ ë¦¬ì„œì¹˜"],
            index=0,
            help="ì¼ë°˜ ëŒ€í™”: ë¹ ë¥¸ LLM ì§ì ‘ ì‘ë‹µ\në”¥ ë¦¬ì„œì¹˜: ì›¹ ê²€ìƒ‰ + ì„œë¸Œì—ì´ì „íŠ¸ ì‹¬ì¸µ ì¡°ì‚¬",
        )

        st.divider()

        # API í‚¤ ë””ë²„ê·¸ (ì• 15ì + ë 5ì + ê¸¸ì´)
        st.caption("ğŸ”‘ API í‚¤ ìƒíƒœ")
        for name in ("ANTHROPIC_API_KEY", "TAVILY_API_KEY"):
            val = os.environ.get(name, "")
            if val:
                st.write(f"âœ… `{name}`")
                st.code(f"ì•: {val[:15]}...\në: ...{val[-5:]}\nê¸¸ì´: {len(val)}ì", language=None)
            else:
                st.write(f"âŒ `{name}`: ì—†ìŒ")
        if not all(_api_key_status.values()):
            st.error("API í‚¤ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. Secrets ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")

        st.divider()

        if st.button("ğŸ—‘ï¸ ì±„íŒ… ê¸°ë¡ ì‚­ì œ", use_container_width=True):
            st.session_state.messages = []
            st.session_state.files = {}
            st.rerun()

        st.divider()
        st.header("ğŸ“ ì €ì¥ëœ íŒŒì¼")

        if st.session_state.files:
            for fname, content in st.session_state.files.items():
                with st.expander(fname):
                    st.code(content, language="markdown")
        else:
            st.info("ì•„ì§ ì €ì¥ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    return mode


# â”€â”€ ì¼ë°˜ ëŒ€í™” ì‹¤í–‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _run_normal_chat(history: list[dict]) -> str:
    """LLMì— ì§ì ‘ ì§ˆë¬¸í•˜ì—¬ ë¹ ë¥¸ ì‘ë‹µì„ ë°›ìŠµë‹ˆë‹¤."""
    model = _init_model()
    lc_messages = _to_langchain_messages(history)
    with st.spinner("ğŸ’¬ ë‹µë³€ ìƒì„± ì¤‘..."):
        response = model.invoke(lc_messages)
    if isinstance(response.content, str):
        return response.content
    parts = [
        item["text"]
        for item in response.content
        if isinstance(item, dict) and item.get("type") == "text"
    ]
    return "\n".join(parts) if parts else str(response.content)


# â”€â”€ ë”¥ ë¦¬ì„œì¹˜ ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¬ë°) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _run_deep_research(agent, state: dict) -> tuple[str, dict, list[dict]]:
    """ì—ì´ì „íŠ¸ë¥¼ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì‹¤í–‰í•˜ê³  ì§„í–‰ ìƒí™©ì„ í‘œì‹œí•©ë‹ˆë‹¤.

    Returns:
        (ì‘ë‹µ í…ìŠ¤íŠ¸, ìµœì¢… íŒŒì¼ dict, ì¶œì²˜ ë¦¬ìŠ¤íŠ¸)
    """
    final_state = None
    tool_calls_shown = set()
    files_before = set(state.get("files", {}).keys())

    with st.status("ğŸ” ë”¥ ë¦¬ì„œì¹˜ ì§„í–‰ ì¤‘...", expanded=True) as status:
        for event in agent.stream(state, stream_mode="values"):
            final_state = event

            for msg in event.get("messages", []):
                if not isinstance(msg, AIMessage):
                    continue
                for tc in getattr(msg, "tool_calls", []) or []:
                    tc_id = tc.get("id", "")
                    if tc_id not in tool_calls_shown:
                        tool_calls_shown.add(tc_id)
                        name = tc.get("name", "unknown")
                        args = tc.get("args", {})
                        detail = ""
                        if "query" in args:
                            detail = f' â†’ "{args["query"]}"'
                        elif "description" in args:
                            desc = args["description"]
                            detail = (
                                f" â†’ {desc[:60]}..."
                                if len(desc) > 60
                                else f" â†’ {desc}"
                            )
                        st.write(f"ğŸ”§ `{name}`{detail}")

        status.update(label="âœ… ë¦¬ì„œì¹˜ ì™„ë£Œ", state="complete")

    if final_state is None:
        return "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", state.get("files", {}), []

    response = _extract_ai_response(final_state.get("messages", []))
    files = final_state.get("files", state.get("files", {}))

    new_files = {k: v for k, v in files.items() if k not in files_before}
    sources = _extract_sources(new_files) if new_files else _extract_sources(files)

    return response, files, sources


# â”€â”€ ë©”ì‹œì§€ ë Œë”ë§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _render_message(msg: dict):
    """ë©”ì‹œì§€ í•˜ë‚˜ë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤ (ì¶œì²˜ í¬í•¨)."""
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])
        if msg.get("sources"):
            _render_sources(msg["sources"])
        if msg.get("mode") == "ë”¥ ë¦¬ì„œì¹˜":
            st.caption("ğŸ”¬ ë”¥ ë¦¬ì„œì¹˜")


# â”€â”€ ë©”ì¸ ì•± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    st.title("ğŸ§  Deep Agent ë¦¬ì„œì¹˜ ì±—ë´‡")
    st.caption("ì›¹ ê²€ìƒ‰ Â· ìš”ì•½ Â· ì„œë¸Œì—ì´ì „íŠ¸ ìœ„ì„ ê¸°ëŠ¥ì„ ê°–ì¶˜ ë¦¬ì„œì¹˜ ì—ì´ì „íŠ¸")

    mode = _render_sidebar()

    for msg in st.session_state.messages:
        _render_message(msg)

    placeholder = (
        "ë¦¬ì„œì¹˜í•  ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”..."
        if mode == "ë”¥ ë¦¬ì„œì¹˜"
        else "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."
    )

    if prompt := st.chat_input(placeholder):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            try:
                if mode == "ì¼ë°˜ ëŒ€í™”":
                    response = _run_normal_chat(st.session_state.messages)
                    st.markdown(response)
                    st.session_state.messages.append(
                        {"role": "assistant", "content": response}
                    )

                else:  # ë”¥ ë¦¬ì„œì¹˜
                    agent = _create_agent()
                    agent_state = {
                        "messages": _to_langchain_messages(
                            st.session_state.messages
                        ),
                        "files": st.session_state.files,
                    }

                    response, files, sources = _run_deep_research(
                        agent, agent_state
                    )
                    st.session_state.files = files

                    st.markdown(response)
                    if sources:
                        _render_sources(sources)
                    st.caption("ğŸ”¬ ë”¥ ë¦¬ì„œì¹˜")

                    st.session_state.messages.append(
                        {
                            "role": "assistant",
                            "content": response,
                            "sources": sources,
                            "mode": "ë”¥ ë¦¬ì„œì¹˜",
                        }
                    )

            except Exception as e:
                error_msg = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                st.error(error_msg)
                st.exception(e)
                st.session_state.messages.append(
                    {"role": "assistant", "content": error_msg}
                )


if __name__ == "__main__":
    main()
