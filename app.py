"""
Deep Agent ë¦¬ì„œì¹˜ ì±—ë´‡ - Streamlit êµ¬í˜„

ì›¹ ê²€ìƒ‰, ìš”ì•½, ì„œë¸Œì—ì´ì „íŠ¸ ìœ„ì„ ê¸°ëŠ¥ì„ í¬í•¨í•œ ë¦¬ì„œì¹˜ ì—ì´ì „íŠ¸ì˜ ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤
ê¸°ì¡´ deep_agents_from_scratch íŒ¨í‚¤ì§€ì˜ ëª¨ë“ˆì„ ì¬ì‚¬ìš©í•˜ì—¬ ì½”ë“œ ì¤‘ë³µì„ ì œê±°í•©ë‹ˆë‹¤.

ê¸°ëŠ¥:
- ì¼ë°˜ ëŒ€í™”: LLM ì§ì ‘ ì‘ë‹µ (ë¹ ë¥¸ ë‹µë³€)
- ë”¥ ë¦¬ì„œì¹˜: ì›¹ ê²€ìƒ‰ + ì„œë¸Œì—ì´ì „íŠ¸ ìœ„ì„ (ì‹¬ì¸µ ì¡°ì‚¬, ì¶œì²˜ í¬í•¨)
"""

import os
import re
from pathlib import Path
import streamlit as st
from datetime import datetime
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ: .env (ë¡œì»¬) â†’ Streamlit Cloud secrets (ë°°í¬) ìˆœì„œë¡œ ì‹œë„
load_dotenv(override=True)

# Streamlit Cloud secrets â†’ os.environ ìœ¼ë¡œ ê°•ì œ ì „ë‹¬ (override)
for key in ("ANTHROPIC_API_KEY", "TAVILY_API_KEY"):
    try:
        if key in st.secrets:
            os.environ[key] = st.secrets[key]
    except Exception:
        pass

from tavily import TavilyClient
from langchain.chat_models import init_chat_model
from langchain.agents import create_agent
from langchain_core.messages import HumanMessage, AIMessage

from deep_agents_from_scratch.state import DeepAgentState
from deep_agents_from_scratch.file_tools import ls, read_file, write_file
from deep_agents_from_scratch.todo_tools import write_todos, read_todos
from deep_agents_from_scratch.research_tools import tavily_search, think_tool
import deep_agents_from_scratch.research_tools as _research_tools
from deep_agents_from_scratch.task_tool import _create_task_tool

# íŒ¨í‚¤ì§€ ë‚´ë¶€ì˜ ëª¨ë“ˆ ë ˆë²¨ í´ë¼ì´ì–¸íŠ¸ë¥¼ ì˜¬ë°”ë¥¸ API í‚¤ë¡œ ì¬ì´ˆê¸°í™”
_research_tools.tavily_client = TavilyClient(
    api_key=os.environ.get("TAVILY_API_KEY", "")
)
_research_tools.summarization_model = init_chat_model(
    model="anthropic:claude-3-5-haiku-20241022",
    temperature=0.0,
    api_key=os.environ.get("ANTHROPIC_API_KEY"),
)
from deep_agents_from_scratch.prompts import (
    FILE_USAGE_INSTRUCTIONS,
    RESEARCHER_INSTRUCTIONS,
    SUBAGENT_USAGE_INSTRUCTIONS,
    TODO_USAGE_INSTRUCTIONS,
)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Deep Agent ë¦¬ì„œì¹˜ ì±—ë´‡",
    page_icon="ğŸ§ ",
    layout="wide",
)

# â”€â”€ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "messages" not in st.session_state:
    st.session_state.messages = []  # [{role, content, sources?, mode?}]
if "files" not in st.session_state:
    st.session_state.files = {}
if "research_stage" not in st.session_state:
    st.session_state.research_stage = "idle"  # "idle" | "plan_pending"
if "pending_plan" not in st.session_state:
    st.session_state.pending_plan = ""
if "pending_query" not in st.session_state:
    st.session_state.pending_query = ""


# â”€â”€ ìºì‹œëœ ë¦¬ì†ŒìŠ¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource
def _init_model():
    """ë©”ì¸ LLMì„ í•œ ë²ˆë§Œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    return init_chat_model(
        model="anthropic:claude-sonnet-4-5",
        temperature=0.0,
        api_key=os.environ.get("ANTHROPIC_API_KEY"),
    )


@st.cache_resource
def _create_agent():
    """ë¦¬ì„œì¹˜ ì—ì´ì „íŠ¸ë¥¼ í•œ ë²ˆë§Œ ìƒì„±í•©ë‹ˆë‹¤."""
    model = _init_model()
    now = datetime.now()

    sub_agent_tools = [tavily_search, think_tool]
    built_in_tools = [ls, read_file, write_file, write_todos, read_todos, think_tool]

    research_sub_agent = {
        "name": "research-agent",
        "description": (
            "Delegate research to the sub-agent researcher. "
            "Only give this researcher one topic at a time."
        ),
        "prompt": RESEARCHER_INSTRUCTIONS.format(
            date=now.strftime("%b %-d, %Y %H:%M:%S (%A)")
        ),
        "tools": ["tavily_search", "think_tool"],
    }

    task_tool = _create_task_tool(
        sub_agent_tools, [research_sub_agent], model, DeepAgentState
    )

    all_tools = sub_agent_tools + built_in_tools + [task_tool]

    subagent_instructions = SUBAGENT_USAGE_INSTRUCTIONS.format(
        max_concurrent_research_units=3,
        max_researcher_iterations=3,
        date=now.strftime("%a %b %-d, %Y"),
    )

    system_prompt = "\n\n".join(
        [
            "# TODO MANAGEMENT",
            TODO_USAGE_INSTRUCTIONS,
            "=" * 80,
            "# FILE SYSTEM USAGE",
            FILE_USAGE_INSTRUCTIONS,
            "=" * 80,
            "# SUB-AGENT DELEGATION",
            subagent_instructions,
        ]
    )

    return create_agent(
        model, all_tools, system_prompt=system_prompt, state_schema=DeepAgentState
    )


# â”€â”€ ë¦¬ì„œì¹˜ ê³„íš ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _generate_plan(query: str) -> str:
    """ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°›ì•„ ë¦¬ì„œì¹˜ ê³„íšë§Œ ìƒì„±í•©ë‹ˆë‹¤ (ì‹¤ì œ ë¦¬ì„œì¹˜ëŠ” ìˆ˜í–‰í•˜ì§€ ì•ŠìŒ)."""
    model = _init_model()
    plan_prompt = (
        "ë‹¹ì‹ ì€ ë¦¬ì„œì¹˜ í”Œë˜ë„ˆì…ë‹ˆë‹¤. ì•„ë˜ ì§ˆë¬¸ì— ëŒ€í•´ ë¦¬ì„œì¹˜ ê³„íšë§Œ ì‘ì„±í•˜ì„¸ìš”.\n"
        "ì‹¤ì œ ë¦¬ì„œì¹˜ëŠ” ìˆ˜í–‰í•˜ì§€ ë§ˆì„¸ìš”.\n\n"
        "ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë²ˆí˜¸ ë§¤ê¸´ ë‹¨ê³„ë³„ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”:\n"
        "1. [ë‹¨ê³„ ì„¤ëª…]\n"
        "2. [ë‹¨ê³„ ì„¤ëª…]\n"
        "...\n\n"
        f"ì§ˆë¬¸: {query}"
    )
    with st.spinner("ğŸ“‹ ë¦¬ì„œì¹˜ ê³„íš ìƒì„± ì¤‘..."):
        response = model.invoke([HumanMessage(content=plan_prompt)])
    if isinstance(response.content, str):
        return response.content
    parts = [
        item["text"]
        for item in response.content
        if isinstance(item, dict) and item.get("type") == "text"
    ]
    return "\n".join(parts) if parts else str(response.content)


# â”€â”€ ìœ í‹¸ë¦¬í‹° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _extract_ai_response(messages: list) -> str:
    """ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì—ì„œ ë§ˆì§€ë§‰ AI ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    for msg in reversed(messages):
        if not isinstance(msg, AIMessage) or not msg.content:
            continue
        if isinstance(msg.content, str):
            return msg.content
        parts = [
            item["text"]
            for item in msg.content
            if isinstance(item, dict) and item.get("type") == "text"
        ]
        if parts:
            return "\n".join(parts)
    return "ë¦¬ì„œì¹˜ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ì €ì¥ëœ íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”."


def _to_langchain_messages(history: list[dict]) -> list:
    """Streamlit ì±„íŒ… íˆìŠ¤í† ë¦¬ë¥¼ LangChain ë©”ì‹œì§€ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    return [
        HumanMessage(content=m["content"])
        if m["role"] == "user"
        else AIMessage(content=m["content"])
        for m in history
    ]


def _extract_sources(files: dict) -> list[dict]:
    """íŒŒì¼ë“¤ì—ì„œ ì¶œì²˜(URL, ì œëª©) ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    sources = []
    seen_urls = set()
    for content in files.values():
        url_match = re.search(r"\*\*URL:\*\*\s*(https?://\S+)", content)
        title_match = re.search(r"# Search Result:\s*(.+)", content)
        if url_match:
            url = url_match.group(1)
            if url not in seen_urls:
                seen_urls.add(url)
                title = title_match.group(1).strip() if title_match else url
                sources.append({"title": title, "url": url})
    return sources


LOCAL_SAVE_DIR = Path("research_outputs")


def _save_files_to_disk(files: dict):
    """ê°€ìƒ íŒŒì¼ì‹œìŠ¤í…œì˜ íŒŒì¼ë“¤ì„ ë¡œì»¬ ë””ìŠ¤í¬ì— ìë™ ì €ì¥í•©ë‹ˆë‹¤."""
    if not files:
        return
    LOCAL_SAVE_DIR.mkdir(exist_ok=True)
    for fname, content in files.items():
        safe_name = Path(fname).name  # ê²½ë¡œ íŠ¸ë˜ë²„ì„¤ ë°©ì§€
        filepath = LOCAL_SAVE_DIR / safe_name
        filepath.write_text(content, encoding="utf-8")


def _render_sources(sources: list[dict]):
    """ì¶œì²˜ ëª©ë¡ì„ ë Œë”ë§í•©ë‹ˆë‹¤."""
    if not sources:
        return
    with st.expander(f"ğŸ“š ì¶œì²˜ ({len(sources)}ê±´)", expanded=False):
        for i, src in enumerate(sources, 1):
            st.markdown(f"{i}. [{src['title']}]({src['url']})")


# â”€â”€ ì‚¬ì´ë“œë°” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _render_sidebar() -> str:
    """ì‚¬ì´ë“œë°”ë¥¼ ë Œë”ë§í•˜ê³  ì„ íƒëœ ëª¨ë“œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")

        # ëª¨ë“œ ì„ íƒ
        mode = st.radio(
            "ëŒ€í™” ëª¨ë“œ",
            options=["ì¼ë°˜ ëŒ€í™”", "ë”¥ ë¦¬ì„œì¹˜"],
            index=0,
            help="ì¼ë°˜ ëŒ€í™”: ë¹ ë¥¸ LLM ì§ì ‘ ì‘ë‹µ\në”¥ ë¦¬ì„œì¹˜: ì›¹ ê²€ìƒ‰ + ì„œë¸Œì—ì´ì „íŠ¸ ì‹¬ì¸µ ì¡°ì‚¬",
        )

        st.divider()

        if st.button("ğŸ—‘ï¸ ì±„íŒ… ê¸°ë¡ ì‚­ì œ", use_container_width=True):
            st.session_state.messages = []
            st.session_state.files = {}
            st.session_state.research_stage = "idle"
            st.session_state.pending_plan = ""
            st.session_state.pending_query = ""
            st.rerun()

        st.divider()
        st.header("ğŸ“ ì €ì¥ëœ íŒŒì¼")

        if st.session_state.files:
            for fname, content in st.session_state.files.items():
                with st.expander(fname):
                    st.code(content, language="markdown")
                    st.download_button(
                        label=f"â¬‡ï¸ {fname} ë‹¤ìš´ë¡œë“œ",
                        data=content,
                        file_name=Path(fname).name,
                        mime="text/markdown",
                        key=f"dl_{fname}",
                    )
            st.caption(f"ğŸ“‚ ìë™ ì €ì¥ ê²½ë¡œ: `{LOCAL_SAVE_DIR.resolve()}`")
        else:
            st.info("ì•„ì§ ì €ì¥ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    return mode


# â”€â”€ ì¼ë°˜ ëŒ€í™” ì‹¤í–‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _run_normal_chat(history: list[dict]) -> str:
    """LLMì— ì§ì ‘ ì§ˆë¬¸í•˜ì—¬ ë¹ ë¥¸ ì‘ë‹µì„ ë°›ìŠµë‹ˆë‹¤."""
    model = _init_model()
    lc_messages = _to_langchain_messages(history)
    with st.spinner("ğŸ’¬ ë‹µë³€ ìƒì„± ì¤‘..."):
        response = model.invoke(lc_messages)
    if isinstance(response.content, str):
        return response.content
    parts = [
        item["text"]
        for item in response.content
        if isinstance(item, dict) and item.get("type") == "text"
    ]
    return "\n".join(parts) if parts else str(response.content)


# â”€â”€ ë”¥ ë¦¬ì„œì¹˜ ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¬ë°) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _run_deep_research(agent, state: dict) -> tuple[str, dict, list[dict]]:
    """ì—ì´ì „íŠ¸ë¥¼ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì‹¤í–‰í•˜ê³  ì§„í–‰ ìƒí™©ì„ í‘œì‹œí•©ë‹ˆë‹¤.

    Returns:
        (ì‘ë‹µ í…ìŠ¤íŠ¸, ìµœì¢… íŒŒì¼ dict, ì¶œì²˜ ë¦¬ìŠ¤íŠ¸)
    """
    final_state = None
    tool_calls_shown = set()
    files_before = set(state.get("files", {}).keys())

    with st.status("ğŸ” ë”¥ ë¦¬ì„œì¹˜ ì§„í–‰ ì¤‘...", expanded=True) as status:
        for event in agent.stream(state, stream_mode="values"):
            final_state = event

            for msg in event.get("messages", []):
                if not isinstance(msg, AIMessage):
                    continue
                for tc in getattr(msg, "tool_calls", []) or []:
                    tc_id = tc.get("id", "")
                    if tc_id not in tool_calls_shown:
                        tool_calls_shown.add(tc_id)
                        name = tc.get("name", "unknown")
                        args = tc.get("args", {})
                        detail = ""
                        if "query" in args:
                            detail = f' â†’ "{args["query"]}"'
                        elif "description" in args:
                            desc = args["description"]
                            detail = (
                                f" â†’ {desc[:60]}..."
                                if len(desc) > 60
                                else f" â†’ {desc}"
                            )
                        st.write(f"ğŸ”§ `{name}`{detail}")

        status.update(label="âœ… ë¦¬ì„œì¹˜ ì™„ë£Œ", state="complete")

    if final_state is None:
        return "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", state.get("files", {}), []

    response = _extract_ai_response(final_state.get("messages", []))
    files = final_state.get("files", state.get("files", {}))

    # ì´ë²ˆ ë¦¬ì„œì¹˜ì—ì„œ ìƒˆë¡œ ìƒì„±ëœ íŒŒì¼ì—ì„œë§Œ ì¶œì²˜ ì¶”ì¶œ
    new_files = {k: v for k, v in files.items() if k not in files_before}
    sources = _extract_sources(new_files) if new_files else _extract_sources(files)

    return response, files, sources


# â”€â”€ ë©”ì‹œì§€ ë Œë”ë§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _render_message(msg: dict):
    """ë©”ì‹œì§€ í•˜ë‚˜ë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤ (ì¶œì²˜ í¬í•¨)."""
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])
        if msg.get("sources"):
            _render_sources(msg["sources"])
        if msg.get("mode") == "ë”¥ ë¦¬ì„œì¹˜":
            st.caption("ğŸ”¬ ë”¥ ë¦¬ì„œì¹˜")


# â”€â”€ ë©”ì¸ ì•± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    st.title("ğŸ§  Deep Agent ë¦¬ì„œì¹˜ ì±—ë´‡")
    st.caption("ì›¹ ê²€ìƒ‰ Â· ìš”ì•½ Â· ì„œë¸Œì—ì´ì „íŠ¸ ìœ„ì„ ê¸°ëŠ¥ì„ ê°–ì¶˜ ë¦¬ì„œì¹˜ ì—ì´ì „íŠ¸")

    mode = _render_sidebar()

    # ì±„íŒ… íˆìŠ¤í† ë¦¬ í‘œì‹œ
    for msg in st.session_state.messages:
        _render_message(msg)

    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if mode == "ë”¥ ë¦¬ì„œì¹˜" and st.session_state.research_stage == "plan_pending":
        placeholder = "ìŠ¹ì¸(ì§„í–‰/ë„¤/ok) ë˜ëŠ” ìˆ˜ì • ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”..."
    elif mode == "ë”¥ ë¦¬ì„œì¹˜":
        placeholder = "ë¦¬ì„œì¹˜í•  ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”..."
    else:
        placeholder = "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."

    if prompt := st.chat_input(placeholder):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            try:
                if mode == "ì¼ë°˜ ëŒ€í™”":
                    response = _run_normal_chat(st.session_state.messages)
                    st.markdown(response)
                    st.session_state.messages.append(
                        {"role": "assistant", "content": response}
                    )

                elif st.session_state.research_stage == "idle":
                    # ë”¥ ë¦¬ì„œì¹˜: ê³„íš ìƒì„± ë‹¨ê³„
                    plan = _generate_plan(prompt)
                    plan_message = (
                        f"**ğŸ“‹ ë¦¬ì„œì¹˜ ê³„íš**\n\n{plan}\n\n---\n"
                        "ì´ ê³„íšëŒ€ë¡œ ì§„í–‰í• ê¹Œìš”? "
                        "ìŠ¹ì¸í•˜ë ¤ë©´ **ì§„í–‰/ë„¤/ok** ë“±ì„ ì…ë ¥í•˜ê³ , "
                        "ìˆ˜ì •ì´ í•„ìš”í•˜ë©´ ìˆ˜ì • ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
                    )
                    st.markdown(plan_message)

                    st.session_state.messages.append(
                        {"role": "assistant", "content": plan_message}
                    )
                    st.session_state.research_stage = "plan_pending"
                    st.session_state.pending_plan = plan
                    st.session_state.pending_query = prompt

                else:
                    # ë”¥ ë¦¬ì„œì¹˜: ìŠ¹ì¸/ìˆ˜ì • ì²˜ë¦¬ ë‹¨ê³„
                    approval_keywords = {
                        "ì§„í–‰", "ë„¤", "ì¢‹ì•„", "ã…‡ã…‡", "ok", "yes",
                        "ì‘", "ì¢‹ì•„ìš”", "í™•ì¸", "ã…‡", "ê³ ", "ì‹œì‘",
                    }
                    user_input = prompt.strip().lower()

                    if user_input in approval_keywords:
                        plan = st.session_state.pending_plan
                    else:
                        plan = prompt  # ìˆ˜ì • ë‚´ìš©ì„ ìƒˆ ê³„íšìœ¼ë¡œ ì‚¬ìš©

                    # ì›ë³¸ ì§ˆë¬¸ + í™•ì •ëœ ê³„íšì„ ì—ì´ì „íŠ¸ì— ì „ë‹¬
                    research_prompt = (
                        f"ì‚¬ìš©ì ì§ˆë¬¸: {st.session_state.pending_query}\n\n"
                        f"ë¦¬ì„œì¹˜ ê³„íš:\n{plan}\n\n"
                        "ìœ„ ê³„íšì— ë”°ë¼ ë¦¬ì„œì¹˜ë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”."
                    )

                    agent = _create_agent()
                    # ê³„íš ìŠ¹ì¸ ê³¼ì •ì˜ ëŒ€í™”ëŠ” ì œì™¸í•˜ê³  ë¦¬ì„œì¹˜ í”„ë¡¬í”„íŠ¸ë§Œ ì „ë‹¬
                    agent_state = {
                        "messages": [HumanMessage(content=research_prompt)],
                        "files": st.session_state.files,
                    }

                    response, files, sources = _run_deep_research(
                        agent, agent_state
                    )
                    st.session_state.files = files
                    _save_files_to_disk(files)

                    st.markdown(response)
                    if sources:
                        _render_sources(sources)
                    st.caption("ğŸ”¬ ë”¥ ë¦¬ì„œì¹˜")

                    st.session_state.messages.append(
                        {
                            "role": "assistant",
                            "content": response,
                            "sources": sources,
                            "mode": "ë”¥ ë¦¬ì„œì¹˜",
                        }
                    )

                    # ìƒíƒœ ì´ˆê¸°í™”
                    st.session_state.research_stage = "idle"
                    st.session_state.pending_plan = ""
                    st.session_state.pending_query = ""

            except Exception as e:
                error_msg = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                st.error(error_msg)
                st.exception(e)
                st.session_state.messages.append(
                    {"role": "assistant", "content": error_msg}
                )


if __name__ == "__main__":
    main()
